{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfW_Ou-KVGAa"
   },
   "source": [
    "\n",
    "## FINANCIAL ECONOMETRICS\n",
    "MODULE 5 | LESSON 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxyw4X77VO2I"
   },
   "source": [
    "# **GARCH MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAQ1bo80VOqJ"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  60 minutes |\n",
    "|**Prior Knowledge** | Basics of Time Series, ARIMA model, ARCH model  |\n",
    "|**Keywords** | GARCH(1,1), GARCH(p,q), Engle’s ARCH Lagrange Multiplier (LM) Test, Nyblom Stability Test, Sign Bias Test, Adjusted Pearson Goodness-of-Fit Test  |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cxW8ig2agTq"
   },
   "source": [
    "*In the last lesson, we discussed using the ARCH process to model non-constant variances in financial asset returns. We talked about the common features of financial asset returns, especially the existence of time-varying volatility in the data. We introduced the ARCH process to model non-constant variance specifically. We went through the key properties of the ARCH process. In this lesson, we are going to continue our journey of modeling non-constant variance. We are going to expand ARCH(1) model and introduce a general model: GARCH. We will go through the basics of GARCH and apply GARCH to Google stock returns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from arch import arch_model\n",
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 9)  # Figure size and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "m5_data = pd.read_csv(\"../M5. goog_eur_10.csv\")\n",
    "\n",
    "# Convert date variable to date format\n",
    "m5_data[\"Date2\"] = pd.to_datetime(m5_data[\"Date\"], format=\"%m/%d/%Y\")\n",
    "goog = m5_data.loc[:, [\"Date2\", \"GOOGLE\"]].set_index(\"Date2\")\n",
    "\n",
    "goog[\"GOOGLE_R\"] = np.log(goog[\"GOOGLE\"]).diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH61yZzFagP0"
   },
   "source": [
    "## **1. GARCH Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPRRNycFagMp"
   },
   "source": [
    "### **1.1 GARCH(1,1) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnloYIWAagIw"
   },
   "source": [
    "GARCH stands for generalized autoregressive conditional heteroskedasticity, and it is an extension of the ARCH model. In the variance function of the ARCH model, a lagged variance term is added to the right of the equation to become a GARCH model. We can formally define GARCH(1,1) as follows:\n",
    "\n",
    "$$ r_{t} = \\sigma_{t} e_{t} $$\n",
    "\n",
    "Where $e_{t} \\sim iid N(0,1)$ and \n",
    "\n",
    "$$\\sigma^{2}_{t} = \\alpha_{0} + \\alpha_{1} r^{2}_{t-1} + \\beta_{1} \\sigma^{2}_{t-1} $$ \n",
    "\n",
    "with $\\alpha_{0}>0, \\ 0 \\le \\alpha_{1} < 1$ and $0\\le \\beta_{1} < 1$\n",
    "\n",
    "The requirements of $\\alpha_{0}>0, \\ 0 \\le \\alpha_{1} < 1$ and $0 \\le \\beta_{1} < 1$ will make sure $r_{t}$ is stationary and the variance is non-negative and finite. \n",
    "\n",
    "Structurally, the relationship between GARCH and ARCH models is similar to that between ARMA and AR models. For ARCH(1) model, we can write it as follows:\n",
    "\n",
    "$$ r_{t} = e_{t} \\sqrt{(\\alpha_{0} + \\alpha_{1} r_{t-1}^{2})} $$\n",
    "\n",
    "In the above ARCH(1) model, we see we use $r_{t-1}$ to predict $r_{t}$. This concept is the same as for AR(1) model, so we sometimes call this term the AR part of the ARCH model.\n",
    "\n",
    "Similarly, let’s write a GARCH(1,1) model in the following form:\n",
    "\n",
    "$$ r_{t} = e_{t} \\sqrt{(\\alpha_{0} + \\alpha_{1} r_{t-1}^{2} + \\beta_{1} \\sigma_{t-1}^{2})} $$\n",
    "\n",
    "In the GARCH(1,1) formula, we not only have $r_{t-1}$ to predict $r_{t}$ but also have $\\sigma_{t-1}$ to predict $r_{t}$. The reason why GARCH(1,1) is like ARMA(1,1) is because we have $r_{t-1}$ like AR part and $\\sigma_{t-1}$ like MA part for GARCH. \n",
    "\n",
    "So why do we need the GARCH model? Even though the ARCH model seems to be able to cover all the features of asset returns, it usually requires many parameters to properly model the asset return. In order to tackle this issue, the GARCH model was developed so that there is no need to include too many terms in a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhFZUufQagFr"
   },
   "source": [
    "### **1.2 Properties of GARCH(1,1) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tP5-X8bTagBn"
   },
   "source": [
    "Most of the properties for the GARCH(1,1) model are similar to the ARCH(1,1) model. In this section, we will highlight the key properties for GARCH(1,1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7UZtLOJaf-V"
   },
   "source": [
    "#### **1.2.1 $r_{t}$ is Conditionally Normally Distributed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfFb68iNaf6Z"
   },
   "source": [
    "The above statement follows the same proof as we describe in ARCH(1) model. We can summarize it in the following formula:\n",
    "\n",
    "$$ r_{t}|r_{t-1} \\sim N \\big(0,\\alpha_{0} + \\alpha_{1} r_{t-1}^{2} + \\beta_{1} \\sigma_{t-1}^{2} \\big) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Dk_yZfTaf2n"
   },
   "source": [
    "#### **1.2.2 $r_{t}^{2}$ is An ARMA(1,1) Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q7mLy7Wafzb"
   },
   "source": [
    "We know that GARCH(1,1) is defined as follows:\n",
    "\n",
    "$$r_{t} = \\sigma_{t} e_{t} $$ and \n",
    "\n",
    "$$ \\sigma^{2}_{t} = \\alpha_{0} + \\alpha_{1} r^{2}_{t-1} + \\beta_{1} \\sigma^{2}_{t-1} $$\n",
    "\n",
    "If we subtract the two equations, we can get the following new equation:\n",
    "\n",
    "$$ r_{t}^{2}=\\alpha_{0}+\\alpha_{1}r_{t-1}^{2}+\\beta_{1}\\sigma_{t-1}^{2}+\\nu_{t} $$ \n",
    "\n",
    "where $\\nu_{t}  =\\sigma_{t}^{2} (e_{t}^{2} - 1)$ and $\\nu_{t}$ is white noise.\n",
    "\n",
    "Since $\\nu_{t-1} = \\sigma_{t-1}^{2} (e_{t-1}^{2} - 1)$ and $r_{t-1}^{2} = \\sigma_{t-1}^{2} e_{t-1}^{2}$, we can get $\\sigma_{t-1}^{2} = r_{t-1}^{2}-\\nu_{t-1}$. Plug this last result into the new equation from the subtraction of two equations from the definition, we can get the following equation:\n",
    "\n",
    "$$ r_{t}^{2} = \\alpha_{0} + (\\alpha_{1} + \\beta_{1}) r_{t-1}^{2} + \\nu_{t} - \\beta_{1} \\nu_{t-1} $$\n",
    "\n",
    "The above equation is an ARMA(1,1) with the squared asset return as the AR part and white noise as the MA part. To ensure stationarity, $0 < \\alpha_{1}+\\beta_{1} < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eem5Rp7afwy"
   },
   "source": [
    "#### **1.2.3 $r_{t}$ is White Noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeAjpPTUafus"
   },
   "source": [
    "Using the same calculation from the ARCH model, we can show that GARCH(1,1) is white noise. We can write unconditional $r_{t}$ distribution as follows:\n",
    "\n",
    "$$ r_{t} \\sim \\text{white noise} \\Big(0, \\frac{\\alpha_{0}}{1 - (\\alpha_{1} + \\beta_{1})} \\Big) $$ \n",
    "\n",
    "where $0 < \\alpha_{1} + \\beta_{1} < 1$\n",
    "\n",
    "This shows similar results as the ARCH(1) model. $r_{t}$ is unconditionally homoscedastic when $\\alpha_{1} + \\beta_{1} < 1$. So when $r_{t}$ is stationary $(0 \\le \\alpha_{1} < 1)$, $\\ r_{t}$ is conditionally heteroskedastic but unconditionally homoscedastic. Also, even though we use asset return as an example for GARCH(1,1) here, it was originally applied to the squared error term from a time series regression model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zXTT8SXafsm"
   },
   "source": [
    "#### **1.2.4 GARCH(1,1) Has Infinite Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsjJrbmPafo6"
   },
   "source": [
    "By iterating the variance equation from GARCH(1,1), we can write the equation as follows:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "  \\sigma_{t^{2}} &= \\alpha_{0} + \\alpha_{1} r_{t-1}^{2} + \\beta_{1} \\sigma_{t-1}^{2} \\\\\n",
    "  &= \\alpha_{0} + \\alpha_{1} r_{t-1}^{2} + \\beta_{1} (\\alpha_{0} + \\alpha_{1} + \\beta_{1} \\sigma_{t-2}^{2})  \\\\\n",
    "  &= \\alpha_{0} + \\alpha_{0} \\beta_{1} + \\alpha_{1} r_{t-1}^{2} + \\alpha_{1} \\beta_{1} r_{t-2}^{2} + \\beta_{1}^{2} (\\alpha_{0} + \\alpha_{1} r_{t-3}^{2} + \\beta_{1} \\sigma_{t-3}^{2})  \\\\\n",
    "  &= \\alpha_{0} (1 + \\beta_{1} + \\beta_{1}^{2} + \\beta_{1}^{3} + \\cdots ) + \\alpha_{1} \\sum_{i=1}^{\\infty} \\beta_{1}^{i-1} r_{t-i}^{2}  \n",
    "\\end{align*} $$\n",
    "\n",
    "We can see that $\\sigma_{t^{2}}$ can be expanded as the infinite sum of past asset returns. This means that all the past asset returns still have influence on today’s conditional variance. Alternatively, we can say GARCH(1,1) has infinite memory. All the coefficients should decay in order to maintain stationarity of the GARCH(1,1) process. Because of this feature, the GARCH model is usually better to model time series data than the ARCH(1) model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRd9077oafl9"
   },
   "source": [
    "### **1.3 GARCH($p, q$) Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc9GFJ7fafi2"
   },
   "source": [
    "As with other time series models we have talked about so far, we can easily extend GARCH(1,1) model to GARCH($p,q$) model. \n",
    "\n",
    "We can define GARCH($p,q$) as follows:\n",
    "\n",
    "$$ r_{t} = \\sigma_{t} e_{t} $$\n",
    "\n",
    "where $e_{t} \\sim iid N(0,1)$  and the variance equation is:\n",
    "\n",
    "$$ \\sigma^{2}_{t} = \\alpha_{0} + \\alpha_{1} r^{2}_{t-1} + \\alpha_{2} r_{t-2}^{2} + \\cdots + \\alpha_{p} r_{t-p}^{2} + \\beta_{1} \\sigma^{2}_{t-1} + \\beta_{2} \\sigma^{2}_{t-2} + \\cdots + \\beta_{q} \\sigma^{2}_{t-q} $$\n",
    "\n",
    "where $\\alpha_{0}>0, \\ \\alpha_{i} \\ge 0$ for $i = 1, \\cdots, p$ and $\\beta_{j} \\ge 0$ for $j = 1, \\cdots, q$\n",
    "\n",
    "We can now rewrite the variance equation with a backshift operator as follows:\n",
    "\n",
    "$$ \\sigma^{2}_{t} = \\alpha_{0} + \\alpha(B) r_{t}^{2} + \\beta(B) \\sigma^{2}_{t} $$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ \\alpha(B) = \\alpha_{1} B + \\alpha_{2} B^{2} + \\cdots + \\alpha_{p} B^{p} $$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ \\beta(B) = \\beta_{1} B + \\beta_{2} B^{2} + \\cdots + \\beta_{q}B^{q} $$\n",
    "\n",
    "For the above characteristic equations, the absolute values of the roots of $B$ must be greater than $1$ to ensure that $r_{t}$ is stationary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWtLDinOaffr"
   },
   "source": [
    "## **2. Model Diagnostics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxaPNPgTafcl"
   },
   "source": [
    "There are a number of tests that are usually used to check for ARCH and GARCH models. We will cover some of them in this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkjnSFrEafV9"
   },
   "source": [
    "### **2.1 Ljung-Box Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayLleKBQafSo"
   },
   "source": [
    "We introduced this test in last module. The Ljung-Box test is used to see if there is serial correlation in a time series. We use this test if there is an overall correlation in the residuals or squared residuals from a time series model, including ARCH and GARCH models. The null hypothesis ($H_0$) is there is no serial correlation in the time series. Hence, if the $p$-value $< 0.05$, we will reject the null hypothesis, and the time series has a serial correlation issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzVeeWumafOd"
   },
   "source": [
    "### **2.2 Information Criteria**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRvFy0uCafKe"
   },
   "source": [
    "In the last module, we also introduced Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). These are the metrics used to evaluate overall model specifications. The lower the numbers of AIC and/or BIC, the better the model specifications. There are other information criteria, but we usually focus on AIC or BIC. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EGDCMK_afGF"
   },
   "source": [
    "### **2.3 Log Likelihood Number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9eB_UksafA8"
   },
   "source": [
    "We also use the log likelihood number to evaluate the goodness-of-fit of a model if it is fitted with the maximum likelihood method. The lower the log likelihood number, the better the model fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7HrKiiaae9M"
   },
   "source": [
    "### **2.4 Engle's ARCH Lagrange Multiplier (LM) Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKNr_tgiae5u"
   },
   "source": [
    "Engle's ARCH LM test, or Engle's ARCH test, determines if there is a serial correlation (autocorrelation) issue in the squared residuals of a time series model. The null hypothesis ($H_0$) is that there is no serial correlation in the squared residuals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZTaxP2mae3H"
   },
   "source": [
    "### **2.5 Nyblom Stability Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7cdqzJaezr"
   },
   "source": [
    "This is the test to check if there is a regime change for the time series over time. The test checks if there is any structural change among variables or if coefficients are stable with no variation overtime. The null hypothesis ($H_0$) is that there is no regime change and the variances for coefficients are $0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elipr_eCaewC"
   },
   "source": [
    "### **2.6 Sign Bias Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWz9PNK68Rx4"
   },
   "source": [
    "This is the test to check if shocks not modeled by the model have an impact on volatilities. The sign bias test is used to determine the impact of large and small shocks on volatility. The negative sign bias test is to check if negative shocks not modeled by the model have an impact on volatility. The positive sign bias test is to check if positive shocks not modeled by the model have an impact on volatility. The null hypothesis ($H_0$) is that there is no impact from shocks not modeled by the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHQ49unb8Rrd"
   },
   "source": [
    "### **2.7 Adjusted Pearson Goodness-of-Fit Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpsHLYpw8RiW"
   },
   "source": [
    "This test compares the empirical distribution of standard residuals with the selected theoretical distribution. It is a chi-squared test. The null hypothesis ($H_0$) is that the empirical distribution of the standard residuals is equal to theoretical distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJQ5rmVZ8Rdy"
   },
   "source": [
    "## **3. GARCH Application: Google Stock Return**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la_bXB3V8RZV"
   },
   "source": [
    "In this section, we are going to use Google stock return data from 2016 to 2021 to run a GARCH model. We’ll start by looking into the data first before building the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wk3xPO78RRn"
   },
   "source": [
    "### **3.1 Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1: Google Stock Returns from 2016 to 2021**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Stock Returns from 2016 to 2021\n",
    "plt.plot(goog.GOOGLE_R)\n",
    "plt.ylabel(\"stock price return\")\n",
    "plt.title(\"Plot of 2016-2021 daily Google stock return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4n9ozswT8RBO"
   },
   "source": [
    "In figure 1, we can see Google stock returns are oscillating around $0$. We can see there are periods of time when the volatilities of returns are higher than other times. This pattern of volatility in Google stock return data is the phenomenon that the GARCH model tries to capture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2: Histogram of Google Stock Returns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram Histogram of Google Stock Returns\n",
    "plt.figure(figsize=(11, 5))\n",
    "goog_r = goog.GOOGLE_R.dropna()\n",
    "\n",
    "x = np.linspace(min(goog_r), max(goog_r), len(goog_r))\n",
    "values, bins, _ = plt.hist(goog_r, bins=25)  # Histogram\n",
    "\n",
    "(mu, sigma) = stats.norm.fit(goog_r)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma) * sum(values * np.diff(bins)), \"r\")  # Density\n",
    "\n",
    "plt.title(\"Google Stock Return Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7JbZ8QZ8Q0S"
   },
   "source": [
    "Figure 2 shows the histogram of Google’s stock returns and a normal distribution curve overlaid on it. We can see Google’s stock return distribution has a higher peak and fatter tail than normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3: Normal QQ Plot for Google Stock Returns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal QQ Plot for Google Stock Returns\n",
    "qq = sm.qqplot(goog_r, stats.norm, fit=True, line=\"q\")\n",
    "qq.set_size_inches((7, 5))\n",
    "plt.title(\"Normal QQ Plot for Google Stock Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyxPyfRY8Qq2"
   },
   "source": [
    "In figure 3, we can clearly see that Google stock returns have a fat-tailed distribution compared to normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 4: Google Stock Squared Returns from 2016 to 2021**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Stock Squared Returns\n",
    "plt.plot(goog.GOOGLE_R**2)\n",
    "plt.ylabel(\"square of stock price return\")\n",
    "plt.title(\"Plot of 2016-2021 daily Google stock squared return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrZAi2Ld8Qha"
   },
   "source": [
    "From figure 4, we can see there are serial correlations to the squared return for Google stock. The high returns are usually followed by more volatilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 5: ACF and PACF Plots for Google Stock Returns and Squared Returns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF Plots for Google Stock Returns and Squared Returns\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "sm.graphics.tsa.plot_acf(goog.GOOGLE_R.dropna(), lags=20, ax=ax[0, 0])\n",
    "ax[0, 0].set(title=\"ACF plot of Google stock return\")\n",
    "ax[0, 0].set_ylim([-0.5, 0.5])\n",
    "\n",
    "sm.graphics.tsa.plot_pacf(goog.GOOGLE_R.dropna(), lags=20, ax=ax[0, 1])\n",
    "ax[0, 1].set(title=\"PACF plot of Google stock return\")\n",
    "ax[0, 1].set_ylim([-0.5, 0.5])\n",
    "\n",
    "sm.graphics.tsa.plot_acf(goog.GOOGLE_R.dropna() ** 2, lags=20, ax=ax[1, 0])\n",
    "ax[1, 0].set(title=\"ACF plot of Google stock squared return\")\n",
    "ax[1, 0].set_ylim([-0.5, 0.5])\n",
    "\n",
    "sm.graphics.tsa.plot_pacf(goog.GOOGLE_R.dropna() ** 2, lags=20, ax=ax[1, 1])\n",
    "ax[1, 1].set(title=\"PACF plot of Google stock squared return\")\n",
    "ax[1, 1].set_ylim([-0.5, 0.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMEDMHXY8QYY"
   },
   "source": [
    "In figure 5, ACF and PACF plots for Google stock returns and squared returns all show patterns like the ARMA model. Hence, we can start with the GARCH(1,1) model for model specifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdtLwAcN8QTv"
   },
   "source": [
    "### **3.2 Model Estimation and Diagnostics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with the GARCH(1,1) model using Google stock return data. Figure 6 shows the GARCH(1,1) model result.\n",
    "\n",
    "**Figure 6: GARCH(1,1) Model with Normal White Noise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) Model with Normal White Noise\n",
    "garch11_spec = arch_model(\n",
    "    goog.GOOGLE_R.dropna(),\n",
    "    vol=\"GARCH\",\n",
    "    p=1,\n",
    "    q=1,\n",
    "    mean=\"AR\",\n",
    "    dist=\"Normal\",\n",
    "    rescale=True,\n",
    ")\n",
    "garch11_fit = garch11_spec.fit()\n",
    "garch11_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic tests for GARCH(1,1) Model with Normal White Noise\n",
    "print(\"GARCH(1,1) Model with Normal White Noise\\n\")\n",
    "\n",
    "# Ljung-Box test and the Box-Pierce test\n",
    "print(\"Ljung-Box and Box-Pierce tests on stanrdized residuals\")\n",
    "print(acorr_ljungbox(garch11_fit.std_resid, boxpierce=True))\n",
    "\n",
    "print(\"\\nLjung-Box and Box-Pierce tests on stanrdized squared residuals\")\n",
    "print(acorr_ljungbox(garch11_fit.std_resid**2, boxpierce=True))\n",
    "\n",
    "# ARCH LM test for conditional heteroskedasticity\n",
    "print(\"\\nARCH LM test for conditional heteroskedasticity\")\n",
    "print(garch11_fit.arch_lm_test(standardized=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lceCgNpj-aKg"
   },
   "source": [
    "Figure 6 shows the GARCH(1,1) model with normal white noise. Everything looks good from the model results except that we can see one of the coefficient estimates is not significant. Now let's look on some diagnostic plots. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 7: Model Diagnostic Plots for the GARCH(1,1) Model with Normal White Noise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Diagnostic Plots for the GARCH(1,1) Model with Normal White Noise\n",
    "fig, ax = plt.subplots(4, 3, figsize=(16, 12))\n",
    "\n",
    "# Figure Row 1 Column 1\n",
    "ax[0, 0].plot(goog.GOOGLE_R)\n",
    "ax[0, 0].plot(2.0 * garch11_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 0].plot(-2.0 * garch11_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 0].tick_params(labelrotation=45)\n",
    "ax[0, 0].set_title(\"Series with 2 conditional SD\")\n",
    "ax[0, 0].set_ylabel(\"Return\")\n",
    "\n",
    "# Figure Row 1 Column 2\n",
    "VaR_1 = stats.norm.ppf(0.99)\n",
    "ax[0, 1].plot(goog.GOOGLE_R)\n",
    "ax[0, 1].plot(VaR_1 * garch11_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 1].plot(-VaR_1 * garch11_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 1].tick_params(labelrotation=45)\n",
    "ax[0, 1].set_title(\"Series with 1% VaR Limits\")\n",
    "ax[0, 1].set_ylabel(\"Return\")\n",
    "\n",
    "# Figure Row 1 Column 3\n",
    "ax[0, 2].plot(garch11_fit.conditional_volatility / 100.0)\n",
    "ax[0, 2].set_title(\"Conditional SD\")\n",
    "ax[0, 2].tick_params(labelrotation=45)\n",
    "\n",
    "# Figure Row 2 Column 1\n",
    "sm.graphics.tsa.plot_acf(garch11_fit.resid / 100.0, lags=20, ax=ax[1, 0])\n",
    "ax[1, 0].set_title(\"ACF of Observations\")\n",
    "ax[1, 0].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 2 Column 2\n",
    "sm.graphics.tsa.plot_acf(garch11_fit.resid**2, lags=20, ax=ax[1, 1])\n",
    "ax[1, 1].set_title(\"ACF of Squared Observations\")\n",
    "ax[1, 1].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 2 Column 3\n",
    "sm.graphics.tsa.plot_acf(np.abs(garch11_fit.resid), lags=20, ax=ax[1, 2])\n",
    "ax[1, 2].set_title(\"ACF of Absolute Observations\")\n",
    "ax[1, 2].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 3 Column 1\n",
    "ax[2, 0].xcorr(\n",
    "    garch11_fit.resid**2,\n",
    "    garch11_fit.resid,\n",
    "    usevlines=True,\n",
    "    maxlags=30,\n",
    "    normed=True,\n",
    "    lw=2,\n",
    ")\n",
    "ax[2, 0].set_title(\"Cross-Correlation of Squared Observations \\n vs Actual Observation\")\n",
    "\n",
    "# Figure Row 3 Column 2\n",
    "standaraized_residuals = garch11_fit.std_resid\n",
    "min_val = np.min(standaraized_residuals)\n",
    "max_val = np.max(standaraized_residuals)\n",
    "empirical_density = np.linspace(min_val, max_val, len(standaraized_residuals))\n",
    "ax[2, 1].plot(empirical_density, stats.norm.pdf(empirical_density), lw=1)\n",
    "ax[2, 1].set_title(\"Empirical density of \\n standarized residuals\")\n",
    "\n",
    "# Figure Row 3 Column 3\n",
    "sm.qqplot(garch11_fit.resid, stats.norm, fit=True, line=\"q\", ax=ax[2, 2])\n",
    "ax[2, 2].set_title(\"Normal QQ-plot\")\n",
    "\n",
    "# Figure Row 4 Column 1\n",
    "sm.graphics.tsa.plot_acf(garch11_fit.std_resid, lags=20, ax=ax[3, 0])\n",
    "ax[3, 0].set_title(\"ACF of Standarized Residuals\")\n",
    "ax[3, 0].set_ylim([-0.12, 0.12])\n",
    "\n",
    "# Figure Row 4 Column 2\n",
    "sm.graphics.tsa.plot_acf((garch11_fit.std_resid) ** 2, lags=20, ax=ax[3, 1])\n",
    "ax[3, 1].set_title(\"ACF of Squared Standarized Residuals\")\n",
    "ax[3, 1].set_ylim([-0.12, 0.12])\n",
    "\n",
    "ax[3, 2].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXVd3V40-aBJ"
   },
   "source": [
    "From figure 7, we can see that normal QQ plot for residuals is still showing fat-tailed distribution. With this piece of information, let’s try a GARCH(1,1) model with Student's t-distribution to see if we can improve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 8: GARCH(1,1) Model with Student's t White Noise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) Model with Student's t White Noise\n",
    "garch11_t_spec = arch_model(\n",
    "    goog.GOOGLE_R.dropna(),\n",
    "    vol=\"GARCH\",\n",
    "    p=1,\n",
    "    q=1,\n",
    "    mean=\"AR\",\n",
    "    dist=\"StudentsT\",  # power=2.0,\n",
    "    rescale=True,\n",
    ")\n",
    "garch11_t_fit = garch11_t_spec.fit()\n",
    "garch11_t_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic tests for GARCH(1,1) Model with Student's t White Noise\n",
    "print(\"GARCH(1,1) Model with StudentsT White Noise\\n\")\n",
    "\n",
    "# Ljung-Box test and the Box-Pierce test\n",
    "print(\"Ljung-Box and Box-Pierce tests on stanrdized residuals\")\n",
    "print(acorr_ljungbox(garch11_t_fit.std_resid, boxpierce=True))\n",
    "\n",
    "print(\"\\nLjung-Box and Box-Pierce tests on stanrdized squared residuals\")\n",
    "print(acorr_ljungbox(garch11_t_fit.std_resid**2, boxpierce=True))\n",
    "\n",
    "# ARCH LM test for conditional heteroskedasticity\n",
    "print(\"\\nARCH LM test for conditional heteroskedasticity for\")\n",
    "print(garch11_t_fit.arch_lm_test(standardized=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBuUvDO5-ZlI"
   },
   "source": [
    "In figure 8, now all the coefficient estimates are significant. Let’s check the model diagnostic plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 9: GARCH(1,1) Model with Student's t White Noise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Diagnostic Plots for the GARCH(1,1) Model with Student's t White Noise\n",
    "fig, ax = plt.subplots(4, 3, figsize=(16, 12))\n",
    "\n",
    "# Figure Row 1 Column 1\n",
    "ax[0, 0].plot(goog.GOOGLE_R)\n",
    "ax[0, 0].plot(2.0 * garch11_t_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 0].plot(-2.0 * garch11_t_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 0].tick_params(labelrotation=45)\n",
    "ax[0, 0].set_title(\"Series with 2 conditional SD\")\n",
    "ax[0, 0].set_ylabel(\"Return\")\n",
    "\n",
    "# Figure Row 1 Column 2\n",
    "VaR_1 = stats.t(df=len(goog.GOOGLE_R) - 1).ppf(0.99)\n",
    "# VaR_1 = stats.norm.ppf(0.99)\n",
    "ax[0, 1].plot(goog.GOOGLE_R)\n",
    "ax[0, 1].plot(VaR_1 * garch11_t_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 1].plot(-VaR_1 * garch11_t_fit.conditional_volatility / 100.0, c=\"r\")\n",
    "ax[0, 1].tick_params(labelrotation=45)\n",
    "ax[0, 1].set_title(\"Series with 1% VaR Limits\")\n",
    "ax[0, 1].set_ylabel(\"Return\")\n",
    "\n",
    "# Figure Row 1 Column 3\n",
    "ax[0, 2].plot(garch11_t_fit.conditional_volatility / 100.0)\n",
    "ax[0, 2].set_title(\"Conditional SD\")\n",
    "ax[0, 2].tick_params(labelrotation=45)\n",
    "\n",
    "# Figure Row 2 Column 1\n",
    "sm.graphics.tsa.plot_acf(garch11_t_fit.resid / 100.0, lags=20, ax=ax[1, 0])\n",
    "ax[1, 0].set_title(\"ACF of Observations\")\n",
    "ax[1, 0].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 2 Column 2\n",
    "sm.graphics.tsa.plot_acf(garch11_t_fit.resid**2, lags=20, ax=ax[1, 1])\n",
    "ax[1, 1].set_title(\"ACF of Squared Observations\")\n",
    "ax[1, 1].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 2 Column 3\n",
    "sm.graphics.tsa.plot_acf(np.abs(garch11_t_fit.resid), lags=20, ax=ax[1, 2])\n",
    "ax[1, 2].set_title(\"ACF of Absolute Observations\")\n",
    "ax[1, 2].set_ylim([-0.3, 0.3])\n",
    "\n",
    "# Figure Row 3 Column 1\n",
    "ax[2, 0].xcorr(\n",
    "    garch11_t_fit.resid**2,\n",
    "    garch11_t_fit.resid,\n",
    "    usevlines=True,\n",
    "    maxlags=30,\n",
    "    normed=True,\n",
    "    lw=2,\n",
    ")\n",
    "ax[2, 0].set_title(\"Cross-Correlation of Squared Observations \\n vs Actual Observation\")\n",
    "\n",
    "# Figure Row 3 Column 2\n",
    "standaraized_residuals = garch11_t_fit.std_resid\n",
    "min_val = np.min(standaraized_residuals)\n",
    "max_val = np.max(standaraized_residuals)\n",
    "empirical_density = np.linspace(min_val, max_val, len(standaraized_residuals))\n",
    "ax[2, 1].plot(empirical_density, stats.norm.pdf(empirical_density), lw=1)\n",
    "ax[2, 1].set_title(\"Empirical density of \\n standarized residuals\")\n",
    "\n",
    "# Figure Row 3 Column 3\n",
    "sm.qqplot(garch11_t_fit.resid, stats.t, fit=True, line=\"q\", ax=ax[2, 2])\n",
    "ax[2, 2].set_title(\"StudentsT QQ-plot\")\n",
    "\n",
    "# Figure Row 4 Column 1\n",
    "sm.graphics.tsa.plot_acf(garch11_t_fit.std_resid, lags=20, ax=ax[3, 0])\n",
    "ax[3, 0].set_title(\"ACF of Standarized Residuals\")\n",
    "ax[3, 0].set_ylim([-0.12, 0.12])\n",
    "\n",
    "# Figure Row 4 Column 2\n",
    "sm.graphics.tsa.plot_acf((garch11_t_fit.std_resid) ** 2, lags=20, ax=ax[3, 1])\n",
    "ax[3, 1].set_title(\"ACF of Squared Standarized Residuals\")\n",
    "ax[3, 1].set_ylim([-0.12, 0.12])\n",
    "\n",
    "ax[3, 2].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lu9QIZs7_hQc"
   },
   "source": [
    "From figure 9, we can see the StudentsT QQ plot shows the model is a good fit. Hence, GARCH(1,1) with Student's t white noise fits the model pretty well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojZk_Lm-_hGT"
   },
   "source": [
    "### **3.3 Model Forecast**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 10: 20 Steps Ahead Forecast for GARCH(1,1) Model with Student's t White Noise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Steps Ahead Forecast for GARCH(1,1) Model with Student's t White Noise\n",
    "\n",
    "# set horizon and forecast\n",
    "horizon = 20\n",
    "garch_forecast = garch11_t_fit.forecast(\n",
    "    reindex=False, horizon=horizon, method=\"simulation\"\n",
    ")\n",
    "\n",
    "# reindex data\n",
    "googr = goog.GOOGLE_R.copy()\n",
    "googr.index = list(range(len(goog_r) + 1))\n",
    "\n",
    "# forecast mean\n",
    "forc_mean = pd.Series(garch_forecast.mean.dropna().squeeze())\n",
    "forc_mean.index = list(range(len(googr), len(googr) + horizon))\n",
    "\n",
    "# volatility forecast\n",
    "variance_fct = pd.DataFrame(data={\"Forecast\": garch_forecast.variance.values[0]})\n",
    "variance_fct.index = list(range(len(googr), len(googr) + horizon))\n",
    "std_fct = [(variance_fct.values[i] * (i + 1)) ** 0.5 for i in range(len(variance_fct))]\n",
    "volatility_fct = pd.DataFrame(np.sqrt(std_fct))\n",
    "\n",
    "# upper/lower bands\n",
    "upper_band = googr.values[-1] * (1.0 + 2 * volatility_fct)\n",
    "upper_band.index = variance_fct.index\n",
    "lower_band = googr.values[-1] * (1.0 - 2 * volatility_fct)\n",
    "lower_band.index = variance_fct.index\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.xlim(1400, len(googr) + 21)\n",
    "plt.ylim(-0.05, 0.05)\n",
    "googr.plot(ylabel=\"Google Log Return\", title=\"Forecast Volatility\")\n",
    "plt.plot((upper_band + lower_band) / 2, \"r--\")\n",
    "plt.fill_between(\n",
    "    upper_band.index.tolist(),\n",
    "    upper_band.values.T[0],\n",
    "    lower_band.values.T[0],\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0266p3pH_gxq"
   },
   "source": [
    "In figure 10, we plot the 20 steps ahead forecast for Google stock return data based on the GARCH(1,1) with Student's t white noise. As the forecast goes farther out, we can see that the 1-sigma bands get wider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB-Lfa0g_gtN"
   },
   "source": [
    "## **4. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dj0ToXda_gk6"
   },
   "source": [
    "In this lesson, we introduced the GARCH model. We first gave the definition of GARCH(1,1) model. We then went through the properties of the GARCH(1,1) model. We also briefly expanded the GARCH model to a higher order GARCH($p,q$) model. Next, we talked about estimation of the GARCH model and the tests for model diagnostics. We finished the lesson with a GARCH application to Google stock returns. In the next lesson, we will go over some details about estimation methods for the GARCH model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright © 2022 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSJmiWK3vDrHpV/qj9NHL8",
   "name": "Lesson Notes - ECON_Module 5_Lesson 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
